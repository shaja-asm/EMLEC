{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaja-asm/EMLEC/blob/dev/notebooks/S1/Session_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGHnHFav4lGi"
      },
      "source": [
        "# Projection, Layernorm, Dropout\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/SkillSurf/introduction_genAI/blob/main/notebooks/S1/Session_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<br />\n",
        "<br />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TzvEm56B4cd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhgC0PRC5a8Q"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-hPaOJH5aFU",
        "outputId": "9c21b1bd-35da-4b87-d66a-32a3d26a059a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b7180220150>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''Hyperparameters for smaller model'''\n",
        "\n",
        "# B = 32 # B: how many independent sequences will we process in parallel?\n",
        "# T = 8  # T: what is the maximum context length for predictions?\n",
        "# C = 32 # C: numer of different features analysed (also D = dims)\n",
        "# H = 4  # H: number of attention heads\n",
        "# L = 4  # L: Number of layers\n",
        "# learning_rate = 1e-3\n",
        "\n",
        "'''Final Hyperparameters'''\n",
        "\n",
        "B = 64 # B: how many independent sequences will we process in parallel?\n",
        "T = 256  # T: what is the maximum context length for predictions?\n",
        "H = 6\n",
        "C = 64*H\n",
        "L = 6\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Common Hyperparameters\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFQvyFf4taN"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwgGJ0-K4rx2",
        "outputId": "4a39c608-92d7-4c3e-89a8-4ccbf8261bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-15 05:44:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-07-15 05:44:55 (28.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbw16RqT4wFg",
        "outputId": "02646c4e-184e-46d0-b90a-53e24f513f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 65\n",
            "vocabulary: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "chars_str = ''.join(chars)\n",
        "print(f'vocab_size: {vocab_size}')\n",
        "print(f'vocabulary: {chars_str}')\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - T, (B,))\n",
        "    x = torch.stack([data[i:i+T] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+T+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djRweqFN43In"
      },
      "source": [
        "### Head, MHSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "84SgLLFc4xt6"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self attention\"\"\"\n",
        "\n",
        "    def __init__(self, Ci, Co):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(Ci, Co, bias=False)\n",
        "        self.query = nn.Linear(Ci, Co, bias=False)\n",
        "        self.value = nn.Linear(Ci, Co, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, Ci  = x.shape\n",
        "        '''\n",
        "        B  - batch               # of independant vectors processed\n",
        "        T  - time/block/context  # of tokens in a context\n",
        "        Ci - channals/dims input # of features in input\n",
        "        '''\n",
        "\n",
        "        k = self.key(x)   # (B,T,Co)\n",
        "        q = self.query(x) # (B,T,Co)\n",
        "\n",
        "        # compute attention scores / affinities\n",
        "        wei = q @ k.transpose(-2,-1)                                 # (B,T,Co) @ (B,Co,T) -> (B,T,T)\n",
        "        wei /= C**0.5                                                # (B,T,T) scaling, bring variance to 1, to prevent softmax clipping\n",
        "        wei  = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))   # (B,T,T) Replace upper triangular of wei with -inf\n",
        "        wei  = F.softmax(wei, dim=-1)                                # (B,T,T) -inf -> 0, rest normalized to 1\n",
        "\n",
        "        v = self.value(x)  # (B,T,Co)\n",
        "        out = wei @ v      # (B,T,T) @ (B,T,Co) = (B,T,Co)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, Ci, H, head_size):\n",
        "        super().__init__()\n",
        "        # 4 heads of 8-dimensional self-attention, for n_embed=32, like a group convolution\n",
        "        self.heads = nn.ModuleList([Head(Ci=Ci, Co=head_size) for _ in range(H)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om6pUr4N463t"
      },
      "source": [
        "### Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2m3aC0tt44sv"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    ''' Transformer block: communication followed by computation '''\n",
        "\n",
        "    def __init__(self, C, H): # C: embedding dimension, H: number of heads\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(C)   # Layernorm along channels (batch & time are batch dims): y = beta + gamma * [x-E(x)]/sqrt(V(x) + ep)\n",
        "        self.sa = MultiHeadAttention(Ci=C, H=H, head_size=C//H)\n",
        "        self.ln2 = nn.LayerNorm(C)\n",
        "        self.ffwd = nn.Sequential(         # Feedforward network, so the tokens can \"think about\" what they found in attention.\n",
        "            nn.Linear(C, C*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(C*4, C),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Residual connections around MSA & FF, to help training\n",
        "        # Note: input without layernorm is added to output\n",
        "\n",
        "        x_skip = x\n",
        "\n",
        "        x = self.ln1(x)\n",
        "        x = self.sa(x)   # (B,T,C), Multi head self attention\n",
        "        x = x + x_skip\n",
        "\n",
        "        x = self.ln2(x)\n",
        "        x = self.ffwd(x) # (B,T,C), Per token level. B,T act as batch dimensions\n",
        "        x = x + x_skip\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEZ6CZrm5ADW"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8B84EGMO49AA"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, B,T,C,H,L):\n",
        "        super().__init__()\n",
        "        self.B, self.T, self.C, self.H, self.L = B,T,C,H,L\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, C) # for every possible token, weights for next token\n",
        "        self.position_embedding_table = nn.Embedding(T, C)\n",
        "\n",
        "        self.blocks  = nn.Sequential(*[Block(C, H) for _ in range(L)])\n",
        "        self.ln_final = nn.LayerNorm(C)\n",
        "        self.lm_head = nn.Linear(C, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)                                    # (B,T,C=n_embed)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(self.T, device=device)) # (T,C): [0,1,2..T-1]\n",
        "\n",
        "        x = tok_emb + pos_emb     # (B,T,C)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_final(x)      # Layernorm applied before last\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):                        # idx is (B, T) array of indices in the current context\n",
        "            idx_cond = idx[:, -self.T:]                        # crop the last block_size tokens for input\n",
        "            logits, loss = self(idx_cond)                      # get the predictions\n",
        "            logits = logits[:, -1, :]                          # (B,T,C) -> (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)                  # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution acc to prob (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)            # New idx is concat (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(B,T,C,H,L)\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqlT4Bwa5Dfo"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pDDJiTV5BPa",
        "outputId": "17a301af-5eea-4618-ae98-0b6faccc88f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3603, val loss 4.3588\n",
            "step 500: train loss 2.4079, val loss 2.4304\n",
            "step 1000: train loss 2.1199, val loss 2.1683\n",
            "step 1500: train loss 1.8473, val loss 1.9691\n",
            "step 2000: train loss 1.6748, val loss 1.8351\n",
            "step 2500: train loss 1.5651, val loss 1.7495\n",
            "step 3000: train loss 1.4946, val loss 1.6948\n",
            "step 3500: train loss 1.4351, val loss 1.6495\n",
            "step 4000: train loss 1.3968, val loss 1.6199\n",
            "step 4500: train loss 1.3639, val loss 1.5965\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:   # every once in a while evaluate the loss on train and val sets\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')     # sample a batch of data\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLhWVvJR5Hmv"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oArZB_RT5FHn",
        "outputId": "d3e4b918-6396-4d12-ab24-f2c901137eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                                                                                                                                II\n",
            "BARENVILISHABREREY:\n",
            "WI Oddist ESWARYARDWARELI:\n",
            "MARDUR EDWARET:\n",
            "Ns'Yen:\n",
            "My gooes to HERARGBAREND:\n",
            "Sold, I live them born has fory,\n",
            "And shall take your a feed yet.\n",
            "\n",
            "KING EDWARD IVI:\n",
            "Ghaod, sir, that is not thou orwh distress.\n",
            "Brack me night no my wood resolable!-Grous\n",
            "ANd, sir. I well then give might ewn you.\n",
            "\n",
            "FRIAR LAUREY:\n",
            "No, safir, if this is another is that will,\n",
            "So susped title this, issafely being desced\n",
            "To stam'd in the sunely of bloods' lay.\n",
            "\n",
            "GREENCET:\n",
            "And will the dignife, that I bear a\n",
            "Poor and then to corns executeouty of him.\n",
            "Nay, they lory this minds a beauted to:\n",
            "To be genter fore heart, madatul,\n",
            "About of the awn behan Trust the keep it,\n",
            "To GREN BOLIZETH:\n",
            "My lords, I will be neeces was feet-gen?\n",
            "What think!\n",
            "\n",
            "Servant:\n",
            "Sel, for thy would that swain you despition\n",
            "You have many you of Jouliets,\n",
            "Throws mad my will dreams sthall by his\n",
            "Aff their some honouratce's upon young.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Sistaps the will be not hance corlowed.\n",
            "\n",
            "PALILIS:\n",
            "I had now our men,\n",
            "The king in our quanch'd knaws to thine.\n",
            "Good with he understaing. What's here;\n",
            "By the afterith and\n",
            "Atill bene they were servet to\n",
            "Which is not tewide our have wooe;\n",
            "ThougJuliet of this stret you well great foul\n",
            "Is any comes and fear's lain-lastle of.\n",
            "Now master a good known a speak itself.\n",
            "What is thou dost that me for thee,\n",
            "Show they friend in helder me elecks to behell.\n",
            "\n",
            "EDWARD:\n",
            "That Backingma-post askide so,\n",
            "As town these upon to our sorrume:\n",
            "Same yor favours berns, in his despering, sies\n",
            "The shap of these blow'd sold:\n",
            "How inseen tomblave and ride chiers.\n",
            "A put on him: I meany person your crownses\n",
            "But had breat again, how we belead; one keep,\n",
            "Furnfewell'n all my back your grace your reyal pourse;\n",
            "Dead not spirit night to still thou, whom they\n",
            "I shall now to an, how news, he hear thy love\n",
            "fiars, as many, that duty both his coldress\n",
            "Where dusts Hastaften that thou would have not;\n",
            "Then, the take your courts was here, and way\n",
            "coven made to show his prevosetch onfound\n",
            "May thy bid Jecus think thorte in y\n"
          ]
        }
      ],
      "source": [
        "context = torch.ones((1, T), dtype=torch.long, device=device)  # start with '\\n\\n\\n\\n' as seed\n",
        "out_ints = m.generate(context, max_new_tokens=2000)[0].tolist() # output list of ints\n",
        "print(decode(out_ints))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fNpRB1u5IuS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}